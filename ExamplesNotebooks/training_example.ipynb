{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "947af978",
   "metadata": {},
   "source": [
    "# Training a new model pipeline example\n",
    "\n",
    "This notebook shows how to train new models based on a h5py file with all preprocessed tensors and a labels folder with generated labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ff5def",
   "metadata": {},
   "source": [
    "## Defining loading and training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e6aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "\n",
    "class H5VideoDataset(Dataset): \n",
    "    def __init__(self, h5_path, labels_dict, transform=None, num_frames=13): \n",
    "        self.h5_path = h5_path \n",
    "        self.labels_dict = {} \n",
    "        self.keys = [] \n",
    "        self.transform = transform \n",
    "        self.num_frames = num_frames \n",
    "        # Open just valid processed video tensors from HDF5\n",
    "        with h5py.File(self.h5_path, \"r\") as f: \n",
    "            for key, label in labels_dict.items(): \n",
    "                if key in f[\"data\"] and f[\"data\"][key].shape == (num_frames, 252, 252, 3): \n",
    "                    self.keys.append(key) \n",
    "                    self.labels_dict[key] = label \n",
    "                    \n",
    "        \n",
    "        print(f\"{len(self.keys)} v√≠deos v√°lidos encontrados de {len(labels_dict)}\") \n",
    "    \n",
    "    def __len__(self): return len(self.keys) \n",
    "        \n",
    "    def __getitem__(self, idx): \n",
    "        key = self.keys[idx] \n",
    "        with h5py.File(self.h5_path, \"r\") as f: \n",
    "            data = np.array(f[\"data\"][key], copy=True, dtype=np.float32) \n",
    "\n",
    "        # Data transformations as pre-trained models expected\n",
    "        video = np.transpose(data, (3,0,1,2)) / 255.0 \n",
    "        mean = np.array([0.45,0.45,0.45], dtype=np.float32) \n",
    "        std = np.array([0.225,0.225,0.225], dtype=np.float32) \n",
    "        video = (video - mean.reshape(3,1,1,1)) / std.reshape(3,1,1,1)\n",
    "        \n",
    "        if self.transform: \n",
    "            video = self.transform(video) \n",
    "            \n",
    "        label = self.labels_dict[key] \n",
    "        \n",
    "        return torch.from_numpy(video).float(), torch.tensor(label, dtype=torch.long)\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for videos, labels in tqdm(loader, desc=\"Treinando\", leave=False):\n",
    "        videos, labels = videos.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(videos)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * videos.size(0)\n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "def eval(model, loader, device, num_classes):\n",
    "    model.eval()\n",
    "    correct_per_class = np.zeros(num_classes)\n",
    "    total_per_class = np.zeros(num_classes)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for videos, labels in tqdm(loader, desc=\"Validando\", leave=False):\n",
    "            videos, labels = videos.to(device), labels.to(device)\n",
    "            outputs = model(videos)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            for i in range(num_classes):\n",
    "                correct_per_class[i] += ((predicted == i) & (labels == i)).sum().item()\n",
    "                total_per_class[i] += (labels == i).sum().item()\n",
    "\n",
    "    acc_per_class = np.divide(correct_per_class, total_per_class, out=np.zeros_like(correct_per_class), where=total_per_class != 0)\n",
    "    precision = np.divide(np.sum(correct_per_class),np.sum(total_per_class))\n",
    "    mean_acc = np.mean(acc_per_class)\n",
    "    return mean_acc, acc_per_class,precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32953f1f",
   "metadata": {},
   "source": [
    "## Loading Dataset and pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d4a844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "55140 v√≠deos v√°lidos encontrados de 55141\n",
      "18961 v√≠deos v√°lidos encontrados de 19053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/gabriel/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "# Define labels folder path\n",
    "labels_path = \"../Common/Labels/Exp_1\"\n",
    "\n",
    "# Define HDF5 dataset path\n",
    "h5_path = \"../Common/Tensors/tensors_strategy_1.h5\"\n",
    "\n",
    "# Define output to save training checkpoints\n",
    "output_path = \"Checkpoints/exp1/\"\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# === Hyperparams ===\n",
    "num_epochs = 20\n",
    "lr = 1e-4\n",
    "patience = 2\n",
    "batch_size = 8 # Adjust based on your system capabilities\n",
    "num_workers = 4 # Adjust based on your system capabilities\n",
    "\n",
    "# Load labels\n",
    "with open(os.path.join(labels_path, \"train_labels.json\"), \"r\") as f:\n",
    "    train_labels = json.load(f)\n",
    "\n",
    "with open(os.path.join(labels_path, \"val_labels.json\"), \"r\") as f:\n",
    "    val_labels = json.load(f)\n",
    "\n",
    "with open(os.path.join(labels_path, \"event_to_idx.json\"), \"r\") as f:\n",
    "    event_to_idx = json.load(f)\n",
    "\n",
    "with open(os.path.join(labels_path, \"idx_to_event.json\"), \"r\") as f:    \n",
    "    idx_to_event = json.load(f)\n",
    "\n",
    "num_classes = len(set(train_labels.values()))\n",
    "\n",
    "# === Datasets e Loaders ===\n",
    "train_dataset = H5VideoDataset(h5_path, train_labels)\n",
    "val_dataset = H5VideoDataset(h5_path, val_labels)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# === Loading pre-treined X3D-S and reseting last 5 layers ===\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)\n",
    "in_features = model.blocks[5].proj.in_features\n",
    "model.blocks[5].proj = nn.Linear(in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# === Loss e Optimizer ===\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63fc7c",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4056526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0.0\n",
    "best_epoch = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "        \n",
    "        print(f\"\\n===== Epoch {epoch}/{num_epochs} =====\")\n",
    "\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        mean_acc, acc_per_class,precision = eval(model, val_loader, device, num_classes)\n",
    "\n",
    "        print(f\"üìâ train loss: {train_loss:.4f}\")\n",
    "        print(f\"üéØ Validation mean accuracy: {mean_acc:.4f}\")\n",
    "        print(f\"General Precision: {precision:.4f}\")\n",
    "        for i, acc in enumerate(acc_per_class):\n",
    "            print(f\"  Class {idx_to_event[str(i)]}: {acc:.4f}\")\n",
    "\n",
    "        # === Salvar checkpoint ===\n",
    "        checkpoint_path = os.path.join(output_path, f\"checkpoint_epoch{epoch+1}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'mean_acc': mean_acc,\n",
    "            'acc_per_class': acc_per_class.tolist(),\n",
    "            'train_loss': train_loss,\n",
    "            'precision': precision,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"üíæ Checkpoint saved on: {checkpoint_path}\")\n",
    "\n",
    "        # === Early Stopping ===\n",
    "        if mean_acc > best_acc:\n",
    "            best_acc = mean_acc\n",
    "            epochs_no_improve = 0\n",
    "            best_epoch = epoch + 1\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"‚èπÔ∏è Early stopping: accuracy did not improve in {patience + 1} epochs.\")\n",
    "                break\n",
    "\n",
    "print(f\"\\nüèÅ Training step ended. Best mean accuracy: {best_acc:.4f}. Beast epoch: {best_epoch}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LinuxEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
